{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NLP for Sentiment analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Importing libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "source": [
    "### Importing dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Wow... Loved this place.'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "dataset.iloc[0,0]"
   ]
  },
  {
   "source": [
    "### Cleaning the texts\n",
    "##### Lower all texts, remove punctuations, apply regular expression syntax, remove stopwords like determinants/articles, apply stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords') No need to download as it takes time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english').remove('not'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(dataset)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset.iloc[i][0])  # pattern, replace_with, source_string\n",
    "    review = review.lower().split()\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # Some words included in stopwords might be required by our corpus as per the context, hence remove them from the stopwords\n",
    "    custom_stopwords = stopwords.words('english')\n",
    "    custom_stopwords.remove('not')\n",
    "    review = [ps.stem(word) for word in review if word not in set(custom_stopwords)]\n",
    "    corpus.append(' '.join(review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['wow love place', 'crust not good', 'not tasti textur nasti', 'stop late may bank holiday rick steve recommend love', 'select menu great price', 'get angri want damn pho', 'honeslti tast fresh', 'potato like rubber could tell made ahead time kept warmer', 'fri great', 'great touch']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0:10:1])"
   ]
  },
  {
   "source": [
    "### Create Bag of Words Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We need a parameter max_features which will use only those words that appear frequently\n",
    "# To get it we first will check how many total words we have after fitting all corpus, and then will decide from that \n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()  # we need a sparse matrix hence convert to toarray()\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "len(X[0])\n",
    "# This number denotes all the words in first review. All words are denoted in binary, if word present, then 1, else 0.\n",
    "# Now we can choose any number from this ie 1563 , lets assume max_features needed will be 1500, add it to CountVectorizer constructor and rerun above code"
   ]
  },
  {
   "source": [
    "### Split data into train and test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "source": [
    "### Training Naive Bayes model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "### Predict Test results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "source": [
    "### Confusion Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[55 32]\n [24 89]]\nAccuracy -  0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(cm)\n",
    "print('Accuracy - ', accuracy_score(y_test, preds))"
   ]
  },
  {
   "source": [
    "### Predicting for new reviews"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['understand filthi kitchen', 'dirt everywher', 'cannot stand longer', 'like', 'love manag make fool us']\n"
     ]
    }
   ],
   "source": [
    "# Using previous stuff like ps, custom_stopwords, etc\n",
    "\n",
    "reviews = ['I understand your filthy kitchen', 'Dirt everywhere', 'Cannot stand longer', 'I like it', 'I love how you managed to make fool of us.']\n",
    "new_reviews = []\n",
    "for review in reviews:\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower().split()\n",
    "    review = [ps.stem(word) for word in review if word not in set(custom_stopwords)]\n",
    "    new_reviews.append(' '.join(review))\n",
    "\n",
    "print(new_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "new_X = cv.transform(new_reviews).toarray()\n",
    "print(model.predict(new_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}